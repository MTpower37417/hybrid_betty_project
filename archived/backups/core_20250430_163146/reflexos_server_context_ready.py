
import os

import uvicorn
from fastapi import FastAPI
from pydantic import BaseModel
from reflexos_memory_core import (load_memory_context,  # memory engine
                                  save_memory)

app = FastAPI()
MEMORY_PATH = os.path.join(os.path.dirname(__file__), "memory")
USER_ID = "user_a"


class UserInput(BaseModel):
    message: str


def mock_gpt_response(context):
    # ‚úÖ ‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å GPT ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ context
    last = context[-1]["content"] if context else "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°"
    return f"ü§ñ ‡∏à‡∏≥‡πÑ‡∏î‡πâ‡∏ß‡πà‡∏≤‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏Ñ‡∏¢‡∏û‡∏π‡∏î‡∏ß‡πà‡∏≤: '{last[:50]}' ... (‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á)"


@app.post("/chat")
async def chat(input: UserInput):
    # ‚ûä load memory stack ‡∏Ç‡∏≠‡∏á user
    memory_stack = load_memory_context(USER_ID)

    # ‚ûã ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô message format ‡∏Ç‡∏≠‡∏á GPT
    messages = memory_stack + [{"role": "user", "content": input.message}]

    # ‚ûå save memory ‡πÄ‡∏Ç‡πâ‡∏≤ stack ‡πÉ‡∏´‡∏°‡πà
    save_memory(input.message, USER_ID)

    # ‚ûç ‡∏ï‡∏≠‡∏ö‡πÅ‡∏ö‡∏ö mock ‡∏ó‡∏µ‡πà‡∏™‡∏∞‡∏ó‡πâ‡∏≠‡∏ô context
    reply = mock_gpt_response(messages)
    return {"reply": reply}

if __name__ == "__main__":
    uvicorn.run("reflexos_server_context_ready:app", host="0.0.0.0", port=5000)
